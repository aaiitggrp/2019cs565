<!DOCTYPE html>
<html>
	<head>
		<title>CS 565</title>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
		<link href = "css/bootstrap.min.css" rel = "stylesheet">
		<link href = "css/styles.css" rel = "stylesheet">
		
				
		
		
	</head>

	<body>

		<div class = "container">
			<div class="row">
				<h3 class = "text-center">CS 565: Intelligent Systems and Interfaces</h3>
				<h4 class = "text-center">Jan-May, 2017</h4>
				<img class="img-thumbnail center-block" src="images/iitg_web_mid.gif" />
				</br>
				
				<!-- <p class = "text-center">Intelligent Systems and Interfaces<p> -->
				
<!-- 				DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING<br>
				INDIAN INSTITUTE OF TECHNOLOGY GUWAHATI<br>
				INDIA - 781039<br>
				PHONE (OFF): +91 361 258 2374<br>
				EMAIL: anand [dot] ashish [at] iitg.ernet.in</p> -->
<!-- 				<div class="col-xs-8 col-xs-offset-2">
					<a href="https://twitter.com/anand_ashish" target="_blank" class="pull-right btn btn-default btn-md"><i class="fa fa-twitter icon-color"></i></a>
					<a href="https://in.linkedin.com/in/anandashish" target="_blank" class="pull-right btn btn-default btn-md"><i class="fa fa-linkedin"></i></a>					
				</div> -->
			</div>
		</div>

		<div class = "navbar navbar-default navbar-static-top">
			<div class = "container">
				<button class = "navbar-toggle" data-toggle = "collapse" data-target = ".navHeaderCollapse">
					<span class = "icon-bar"></span>
					<span class = "icon-bar"></span>
					<span class = "icon-bar"></span>
				</button>

				<div class = "collapse navbar-collapse navHeaderCollapse">
					<ul class = "nav navbar-nav">
						<li><a href = "index.html">Home</a></li>
						<li class = "active"><a href = "resources.html">Resources</a></li>
<!-- 						<li><a href = "assignments.html">Assignments</a></li>
						<li><a href = "courseproject.html">Course Project</a></li> -->
						<!-- <li><a href = "gallery.html">Gallery</a></li> -->

					</ul>
				</div>
			</div>
		</div>
					
		<!-- main content goes here -->
		<!-- everythin after the menu bar -->
		<div class = "container" id="mainContent">

			<!-- <div class="text-center">
				<a class="btn btn-primary" href="#calendar">Calendar</a>
				<a class="btn btn-primary" href="#latestNews">Latest News</a>
			</div>
			<br/> -->
		
		
			<!-- education -->
			<!-- <div class = "row col-xs-10 col-xs-offset-1 col-sm-5 col-sm-offset-1"> -->
				
<!-- 				<p>
					<a href="announcements.html" class="btn btn-success btn-lg"> 
						<span class="glyphicon glyphicon-hand-right"></span> Call for Internship
					</a>
				</p> -->
				
<!-- 				<h3><u>Course Description</u></h3>
				<p>The course can be described with the help of an appropriate diagram in the right. </p>
				<p>Blah blah blah blah blah blah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blahblah blah blah blah</p> -->
				<!-- <ul>
					<li class="text-justify">PhD (School of Electrical and Electronics Engineering), Nanyang Technological University, Singapore (2006-2009)</li>
					<li class="text-justify">Int MSc (Mathematics and Sc Computing), Indian Institute of Technology, Kanpur, India (1997-2002)</li>
				</ul> -->
				
<!-- 				<h3><u>Research Interests</u></h3>
				<p>Systems Biology, Computational Biology, Machine Learning, Clinical Data Mining, Evolutionary Algorithms <a href="research.html">More...</a></p>
				<p><a href="docs/Ashish-cv.pdf" target="_blank">Download CV</a></p> -->

<!-- 			</div> -->

			<!-- image div -->
<!-- 			<div class = "row col-xs-10 col-xs-offset-1 col-sm-5 col-sm-offset-0">
				<img class="img-thumbnail center-block" src="images/Appropriate Diagram.png" />
			</div> -->

			<!-- Table of Contents -->
			<div class = "row col-xs-10 col-xs-offset-1">
			<h3>Table of Contents</h3>
			<p>
			<ul>
				<li class="text-justify"><a href="#syllabus_and_references">Syllabus and References</a></li>
				<li class="text-justify"><a href="#assignments">Assignments</a></li>
				<li class="text-justify"><a href="docs/Project groups_ Presentation schedule - Assignment 1.pdf" target="_blank">Mid-Term Presentation Schedule</a></li>
				<li class="text-justify"><a href="docs/2017-CS565-Final-Presentation-Schedule - Sheet2.pdf" target="_blank">Final Presentation Schedule</a></li>
				<li class="text-justify"><a href="#text_and_reference_books">Text and Reference Book(s)</a></li>
				<li class="text-justify"><a href="#nlp_tools">NLP Tools</a></li>
				<!-- <li class="text-justify"><a href="#related_blogs">Related Blogs</a></li> -->
				<li class="text-justify"><a href="#tut_nlp_python">Tutorials: NLP + Python</a></li>
<!-- 					<ul>
						<li class="text-justify">Tuesday 5 - 6:25 PM</li>
						<li class="text-justify">Wednesday 4 - 4:55 PM</li>
						<li class="text-justify">Thursday 3 - 3:55 PM</li>
						<li class="text-justify">Friday 2 - 2:55 PM</li>
					</ul> -->
				<li class="text-justify"><a href="#similar_courses">Similar Courses</a></li>
				<li class="text-justify"><a href="#nlp_conf_calendar">NLP Conference Calendar</a></li>
			</ul></p>
			</div>

			<!-- Syllabus and References -->
			<div class = "row col-xs-10 col-xs-offset-1">
			<h4 id="syllabus_and_references"><u>Syllabus and References</u></h4>
			<p>
<!-- <div class="bs-example"> -->
    <table class="table table-hover">
	<!-- <caption>Updates after each lecture.</caption> -->
	<!-- Download required icons from https://fortawesome.github.io/Font-Awesome/icons/ -->
	<!-- Use different color codes for different type of events using tr class -->
        <thead>
            <tr class="active">
                <th>Event</th>
                <th>Date</th>
                <th>Description</th>
                <th>References</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Introductory Lecture</td>
                <td>Jan 10</td>
                <td>Course Introduction</td>
				<td>[ <a href="docs/CS565Lec20170110-General-Introduction.pdf" target="_blank">Lecture Slides</a> ]</td>				
            </tr>
			
            <tr>
                <td>Lecture 1</td>
                <td>Jan 11</td>
                <td>Getting Started with NLP</td>
				<td>[ <a href="docs/2017-1-11-Lec-CS565.pdf" target="_blank">Lecture Slides</a> ]<!--<br/>
				[ Video lecture: <a href="https://class.coursera.org/nlangp-001/lecture" target="_blank">Coursera</a> Week 1- Introduction (part 1 and part 2) ]<br/>
[ Text Reference: Chapter 4 - 4.2, FSNLP ]--></td>
                <td> </td>
            </tr>
			
			<tr>
                <td>Lecture 2</td>
                <td>Jan 17</td>
                <td>Words - Collocations</td>
				<td>[ <a href="docs/2017-1-17-Lec-CS565.pdf" target="_blank">Lecture Slides</a> ] <!-- <i class="fa fa-star fa-spin fa-pulse" style="color:red"></i> <i class="fa fa-star" style="color:red"></i> <b><i>Updated on Jan 22</i></b><br/>
[ Text Reference: Chapter 5 - 5.2, FSNLP ]--></td>
                <td> </td>
            </tr>			
			
			<tr>
                <td>Lecture 3</td>
                <td>Jan 18</td>
                <td>Words - Finding Collocations</td>
				<td>[ <a href="docs/2017-1-18-Lec-CS565.pdf" target="_blank">Lecture Slides</a> ]<br/>
[ Text Reference: Maximum Likelihood Estimate: DHS, Chapter 3-3.2 ]</td>
                <td> </td>
            </tr>
			
			<tr>
                <td>Lecture 4</td>
                <td>Jan 24</td>
                <td>Language Modeling</td>
				<td>[ <a href="docs/2017-1-24-Lec-CS565.pdf" target="_blank">Lecture Slides</a> ]<br/>
[ References:  <a href="http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf" target="_blank"> Prof Collins, Columbia University Lecture Notes</a> ]<br/>
[  &nbsp &nbsp  &nbsp &nbsp    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp<a href="http://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank"> SLP (3rd ed.), Chapter 4</a> ]</td>
                <td> </td>
            </tr>

				<tr>
                <td>Lecture 5</td>
                <td>Jan 25</td>
                <td>Smoothing Techniques</td>
				<td>[ Laplace, Add-k, Witten-Bell, Backoff and Interpolation ]</td>
                <td> </td>
            </tr>
			
			<tr class="info">
                <td>Lecture 6</td>
                <td>Jan 27</td>
                <td>Smoothing Techniques</td>
				<td>[Backoff and Interpolation, Absolute Discount] <br/>
 Project guideline <a href="docs/2017-1-27-Project Guideline.pptx" target="_blank">[Slide]</a></td>
                <td> </td>
            </tr>
			
			<tr>
                <td>Lecture 7</td>
                <td>Jan 31</td>
                <td>Introduction to Neural Language Model: improving over n-gram model; <br/> Introduction to Neural Networks</td><td> [<a href="https://goo.gl/VWSJeA" target="_blank">Video Lecture</a>(all 1.*)]</td>
				<!--<td> [ <a href="docs/CS565Lec20160203.pdf" target="_blank">Lecture Slides</a> ]<br/>
				[ Video lecture: <a href="https://class.coursera.org/nlangp-001/lecture" target="_blank">Coursera</a> Week 1 - Parameter Estimation in Language Models ]<br/>
[ <a href="http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf" target="_blank">Prof. Collins Lecture Note</a> ]</td>-->
                <td> </td>
            </tr>
			
			<tr>
                <td>Lecture 8</td>
                <td>Feb 1</td>
                <td>1. Summarizing discussion on probabilistic neural language model with flat and hierarchical output layer <br/> 2. vector semantics</td> <td> [<a href="https://goo.gl/hyLL5D" target="_blank">Video Lecture</a>(all 2.*)] <br/> [<a href="http://web.stanford.edu/~jurafsky/slp3/15.pdf" target="_blank">Vector semantics reference</a>]</td>
				<!--<td>[ Chapters 'N-Grams' and 'Hidden Markov Models', SLP ] <br/><i class="fa fa-star" style="color:red"></i> <b><i>Updated on Feb 9</i></b></td>-->
                <td> </td>
            </tr>
		<tr>
                <td>Lecture 9</td>
                <td>Feb 7</td>
                <td>Neural Network Language Model</td> <td> [<a href="https://goo.gl/UWtRWT" target="_blank">Video Lectures</a>(10.5 [NLP-LM], 10.6 [NNLM] and 10.7 [Hierarchical output layer])]
			<br/>[ Optional Video Lectures: 10.1 - 10.7 ]</td>
				<!--<td>[ Chapters 'N-Grams' and 'Hidden Markov Models', SLP ] <br/><i class="fa fa-star" style="color:red"></i> <b><i>Updated on Feb 9</i></b></td>-->
                <td> </td>
		<tr>
                <td></td>
                <td>Feb 10</td>
                <td>No Class</td>
				<td></td>				
            </tr>
            </tr>
	<tr class="success">
                <td>Hands-on-Session</td>
                <td>Feb 12 <br/>(2 - 5 pm)</td>
                <td>A gentle introduction to Neural Networks and Tensorflow</td>
				<td>[Reference: <a href="docs/hands-on-session.zip" target="_blank">Hands-on-session</a>]</td>
            </tr>
			<tr>
                <td>Lecture 10</td>
                <td>Feb 14</td>
                <td>Vector Semantics: Short and Dense representation (Skip-gram with negative sampling)</td>
				<td>[References: 1. <a href="http://web.stanford.edu/~jurafsky/slp3/16.pdf" target="_blank">Semantics with Dense Vectors</a> ] <br/> [2. <a href="https://goo.gl/P0KSyf" target="_blank">word2vec explained, Goldberg and Levy</a>]</td>				
            </tr>

			<tr>
                <td>Lecture 11</td>
                <td>Feb 15</td>
                <td>GloVe (Global Vectors)</td>
				<td>[Reference: <a href="https://goo.gl/yRc0yn" target="_blank">Global vectors for word representation</a>] <!--[ <a href="http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf" target="_blank">Prof. Collins Lecture Note</a> ]<br/>
				[ Video lecture: <a href="https://class.coursera.org/nlangp-001/lecture" target="_blank">Coursera</a> Week 2 - Tagging Problems, and Hidden Markov Models ]<br/>Optional Reading for HMMs:
					<ul>
					<li>Chapter 'Markov Models', FSNLP</li>
					<li>Chapter 'Hidden Markov Models', SLP</li>
					<li><a href="http://www.robots.ox.ac.uk:5000/~vgg/rg/papers/hmm.pdf" target="_blank">Rabiner Tutorial on HMMs</a></li>
					</ul>-->
				</td>
            </tr>

			<tr>
                <td>Lecture 12</td>
                <td>Feb 17</td>
                <td>1. Skip-gram with negative sampling-implicit matrix factorization<br/>2. Improving vector representation further: Retrofitting and 
Counter-fitting</td>
				<td>References: 1. [ <a href="https://goo.gl/DI0d9d" target="_blank">Neural Word Embedding
as Implicit Matrix Factorization</a> ]<br/>
				[ 2. <a href="https://goo.gl/LcajYX" target="_blank">Retrofitting Word Vectors to Semantic Lexicons</a> <br/> <a href="https://goo.gl/5mbyBB" target="_blank">Counter-fitting Word Vectors to Linguistic Constraints</a>]</td>
            </tr>
		<tr>
			<td>Lecture 13</td>
                <td>Feb 21</td>
                <td>Sequence Tagging Problem and HMM</td>
				<td>[Reference: <a href="docs/hmms-spring2013.pdf" target="_blank">Collins Notes on "Tagging problems and HMM"</a>]</td>
            </tr>
		<tr>
		<td>Lecture 14</td>
                <td>Feb 22</td>
                <td>Sequence Tagging problem and MEMM</td>
				<td>References: Collins Notes on "MEMMS (Log-Linear Tagging Models)" <br/>[1. <a href="docs/fall2014-loglineartaggers.pdf" target="_blank">MEMMs (Log-Linear Tagging Models)</a>]<br/> [<a href="docs/loglinear.pdf" target="_blank">Log-Linear Models</a>]</td>
            </tr>
		<tr class="danger">
                <td>Mid Sem</td>
                <td>Feb 27 - <br/>Mar 5</td>
                <td>To be updated</td>
                <td></td>
            </tr>
		<tr>
		<td>Lecture 15</td>
                <td>Mar 7</td>
                <td>Sequence Tagging problem and Linear Conditional Random Field (CRF)</td>
				<td>[Reference: <a href="docs/crf.pdf" target="_blank">LogLinear Models, MEMMs and CRFs</a>]</td>
            </tr>
		<tr>
		<td>Lecture 16</td>
                <td>Mar 8</td>
                <td>Neural Net architectures for sequence labeling</td>
				<td>[Reference: <a href="docs/collobert11a.pdf" target="_blank">NLP (Almost) from scratch</a>]</td>
            </tr>
		<tr>
		<td>Lecture 17</td>
                <td>Apr 4</td>
		<td>Syntactic Parsing</td>
               <td>References: 1. [ <a href="docs/slp-3rded-ch12-syntacting-parsing.pdf" target="_blank">SLP-3rd ed-chapter 12</a> ]<br/>
				[ 2. <a href="docs/pcfgs.pdf" target="_blank">Collins notes on PCFGs</a>]</td>
            </tr>
			
			<!--<tr class="success">
                <td>Reading Assignment</td>
                <td>Feb<br/>17-19</td>
                <td>Parameter Estimation in Log-linear Models</td>
				<td>[ Section 7, <a href="http://www.cs.columbia.edu/~mcollins/loglinear.pdf" target="_blank">Prof. Collins Lecture Note</a> ]</td>
            </tr>
			
			<tr>
                <td>Lecture 11</td>
                <td>Feb 25</td>
                <td>Log-Linear Models for Tagging (MEMMs)</td>
				<td>[ <a href="https://d396qusza40orc.cloudfront.net/nlangp/loglinear-tagging.pdf" target="_blank">Prof. Collins Lecture Slides</a> ]<br/>
				[ Video lecture: <a href="https://class.coursera.org/nlangp-001/lecture" target="_blank">Coursera</a> Week 8 - Log-linear Models for Tagging (MEMMs) ]<br/>Relevant Lecture Notes
:
					<ul>
					<li><a href="http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf" target="_blank">MEMMs (Log-Linear Tagging Models)</a></li>
					<li><a href="http://www.cs.columbia.edu/~mcollins/crf.pdf" target="_blank">Log-Linear Models, MEMMs, and CRFs</a></li>
					</ul>
				</td>
            </tr>

			<tr>
                <td>Lectures<br/>12-18</td>
                <td>Mar<br/> 9-11,<br>16, 19,<br/>22, 24</td>
                <td>Introduction to Neural Networks</td>
				<td>[ <a href="docs/CS565Lec20160309.pdf" target="_blank">Lecture Note</a> ]</td>
            </tr>
			
			<tr>
                <td>Lectures<br/>19-20</td>
                <td>Mar<br/>30-31</td>
                <td>Vector Semantics</td>
				<td>[ <a href="docs/CS565Lec20160330.pdf" target="_blank">Lecture Slides</a> ]</br>
				[ <a href="docs/word2vec_negative_sampling_explained.pdf" target="_blank">word2vec Explained</a>: Negative Sampling Word Embedding ]</br>
				[ Neural Network Language Model:<a href="http://www.youtube.com/watch?v=FoDz01QNSiY" target="_blank"> Video</a>, <a href="https://dl.dropboxusercontent.com/u/19557502/10_06_neural_network_language_model.pdf" target="_blank"> Slides</a> ]</br>
				[ Hierarchical Output layer:<a href="http://www.youtube.com/watch?v=B95LTf2rVWM" target="_blank"> Video</a>, <a href="https://dl.dropboxusercontent.com/u/19557502/10_07_hierarchical_output_layer.pdf" target="_blank"> Slides</a> ]<br/>
				[ Chapter 'Vector Semantics', SLP:<a href="docs/SLP3/Ch19.pdf" target="_blank"> Chapter</a>, <a href="docs/SLP3/Ch19_Slides.pdf" target="_blank"> Slides</a> ]
				</td>
            </tr> -->

			
            <!-- <tr class="active">
                <td>Assignment 1</td>
                <td>Due date: Feb 7</td>
                <td>[ <a href="docs/2017-1-29-Assignment-1-CS565.pdf"target="_blank">Questions</a> ]</td>
                <td></td>
            </tr>
            <tr>
                <td>Lecture 3</td>
                <td></td>
                <td>To be updated</td>
                <td> </td>
            </tr>			
			<tr class="danger">
                <td>Mid Sem</td>
                <td></td>
                <td>To be updated</td>
                <td></td>
            </tr>
            <tr>
                <td>Lecture 4</td>
                <td></td>
                <td>To be updated</td>
                <td> </td>
            </tr>			
			<tr class="info">
                <td>Project Final Presentation</td>
                <td></td>
                <td>To be updated</td>
                <td></td>
            </tr>
			<tr  class="danger">
                <td>End Sem</td>
                <td></td>
                <td>To be updated</td>
                <td></td>
            </tr> -->
			<tr  class="active">
                <td> </td>
                <td> </td>
                <td> </td>
                <td> </td>
            </tr>		
        </tbody>
		
		<!--<tfoot>
		            <tr class="active">
                <th>End</th>
                <th></th>
                <th></th>
                <th></th>
            </tr>
		</tfoot> -->
    </table>
 </div>
			<!--</p> 
			</div>	-->	
			
			
			<!-- Related Blogs -->
			<!-- <div class = "row col-xs-10 col-xs-offset-1">
			<h4 id="related_blogs"><u>Related Blogs</u></h4>
			<p>
				<Ol>
					<li class="text-justify"><b><a href="http://colah.github.io/about.html" target="_blank">Christopher Olah:</a></b> <a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank">Calculus on Computational Graphs: Backpropagation</a></li>
					<li class="text-justify"><b><a href="https://twitter.com/m0_z" target="_blank">Michael Czerny</a>:</b> <a href="https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis" target="_blank">Modern Methods for Sentiment Analysis (Word2Vec and Doc2Vec with gensim)</a></li>
					<li class="text-justify"><b><a href="https://twitter.com/dennybritz" target="_blank">Denny Britz</a>:</b> <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank">Understanding Convolutional Neural Networks (CNN) for NLP</a></li>					
					<li class="text-justify"><b><a href="https://twitter.com/dennybritz" target="_blank">Denny Britz</a>:</b> <a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/" target="_blank">Implementing a Convolutional Neural Network (CNN) for Text Classification in TensorFlow</a></li>
					<li class="text-justify"><b><a href="http://colah.github.io/about.html" target="_blank">Christopher Olah:</a></b> <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Understanding Long Short-Term Memory (LSTM) Networks</a></li>
					<li class="text-justify"><b><a href="http://colah.github.io/about.html" target="_blank">Christopher Olah:</a></b> <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" target="_blank">Deep Learning, NLP, and Representations</a></li>
					<li class="text-justify"><b><a href="https://twitter.com/dennybritz" target="_blank">Denny Britz</a>:</b> <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank">Introduction to Recurrent Neural Networks (RNNs)</a></li>
					
				</Ol>
			</p>
			</div> -->

			
			<!-- Assignments -->
						<div class = "row col-xs-10 col-xs-offset-1">
						<h4 id="assignments"><u>Assignments</u></h4>
						<p>
							<Ol>
								<li class="text-justify"><b>Assignment 1 (Due date: 07/02/2017):</b> <a href="docs/2017-1-29-Assignment-1-CS565.pdf"target="_blank"><u>Questions</u></a>&nbsp;&nbsp;&nbsp;<u>Solutions</u></li>
								<li class="text-justify"><b>Assignment 2 (Due date: 10/03/2017):</b> <a href="docs/assignment2.pdf"target="_blank"><u>Questions</u></a>&nbsp;&nbsp;&nbsp;<u>Solutions</u></li>
								
							</Ol>
						</p>
						</div>

			
			<!-- End Assignments -->
			



			<!-- Text and Reference Book(s) -->
			<div class = "row col-xs-10 col-xs-offset-1">
			<h4 id="#text_and_reference_books"><u>Text and Reference Book(s)</u></h4>
			<p>
				<Ol>
					<li class="text-justify"><b>FSNLP</b>: Chris Manning and Hinrich Sch&uuml;tze. <b><i>Foundations of Statistical Natural Language Processing</i></b>. MIT Press, Cambridge, MA: May 1999. <a href="http://nlp.stanford.edu/fsnlp/" target="_blank"><u>Companion Website</u></a></li>
					<li class="text-justify"><b>DHS</b>: Duda, Richard O., Peter E. Hart, and David G. Stork. <b><i>Pattern Classification</i></b>. John Wiley & Sons, 2012. <a href="http://as.wiley.com/WileyCDA/Section/id-105036.html" target="_blank"><u>Companion Website</u></a></li>
					<li class="text-justify"><b>SLP</b>: Jurafsky, Dan, and James H. Martin. <b><i>Speech and Language Processing</i></b>. Pearson Education India, 2000. <a href="http://web.stanford.edu/~jurafsky/slp3/" target="_blank"><u>Companion Website</u></a></li>
					<li class="text-justify"><b>NNLM</b>: Simon O. Haykin. <b><i>Neural Networks and Learning Machines</i></b>. Pearson Education India, 2009. <a href="https://media.pearsoncmg.com/bc/abp/engineering-resources/products/product.html#product,isbn=0131471392" target="_blank"><u>Companion Website</u></a></li>
				</Ol>
			</p>
			</div>
			<!-- End Text and Reference Books -->
			
			<!-- NLP Tools -->
			<div class = "row col-xs-10 col-xs-offset-1">
			<h4 id="nlp_tools"><u>NLP Tools</u></h4>
			<p>
				<Ol>
					<li class="text-justify"><b>Five open source NLP tools:</b> <a href="https://opensource.com/business/15/7/five-open-source-nlp-tools" target="_blank"><u>Link</u></a></li>
					<li class="text-justify"><b>Tools for different NLP tasks: </b><a href="http://www.phontron.com/nlptools.php" target="_blank"><u>Link</u></a></li>
				</Ol>
			</p>
			</div>
			<!-- End NLP Tools -->
			
			
			
			
			
			<!-- Tutorials: NLP + Python -->
			<div class = "row col-xs-10 col-xs-offset-1">
			<h4 id="tut_nlp_python"><u>Tutorials: NLP + Python</u></h4>
			<p>
				<Ol>
					<li class="text-justify"><b>Natural language Toolkit (NLTK) Tutorial:</b> <a href="http://www.nltk.org/book/" target="_blank"><u>Book</u></a> <a href="docs/NLTK_basic_Dragomir.pdf" target="_blank"><u>Set Up</u></a></li>
					<li class="text-justify"><b>Python Numpy Tutorial: </b><a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank"><u>Stanford CS231n</u></a></li>
					<li class="text-justify"><b>python-crfsuite Tutorial: </b><a href="http://python-crfsuite.readthedocs.org/en/latest/" target="_blank"><u>Official Homepage</u></a></li>
					<li class="text-justify"><b>Theano Tutorial: </b><a href="http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/" target="_blank"><u>Speeding up your Neural Network with Theano and the GPU</u></a></li>					
				</Ol>
			</p>
			</div>

			<!-- Similar Courses -->
			<div class = "row col-xs-10 col-xs-offset-1">
			<h4 id="similar_courses"><u>Similar Courses</u></h4>
			<p>
				<Ol>
					<li class="text-justify"><a href="http://www.cs.columbia.edu/~mcollins/teaching.html" target="_blank">Columbia University, Advanced NLP by Prof. Collins</a></li>
					<li class="text-justify"><a href="http://cs224d.stanford.edu/" target="_blank">Stanford University, Deep Learning for Natural Language Processing</a></li>
					<li class="text-justify"><a href="http://web.stanford.edu/class/cs224u/" target="_blank">Stanford University, Natural Language Understanding</a></li>
					<li class="text-justify"><a href="http://www.cse.iitd.ac.in/~mausam/courses/csl772/autumn2014/" target="_blank">IIT Delhi, NLP by Dr. Mausam</a></li>					
					<li class="text-justify"><a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html" target="_blank">Stanford University, Convolutional Neural Networks for Visual Recognition</a></li>

					<li class="text-justify"><a href="http://web.stanford.edu/class/cs224n/" target="_blank">Stanford University, Natural Language Processing with Deep Learning</a></li>
				</Ol>
			</p>
			</div>

			<!-- NLP Conference Calendar -->
			<div class = "row col-xs-10 col-xs-offset-1">
			<h4 id="nlp_conf_calendar"><u>NLP Conference Calendar</u></h4>
			<p>
			Click <a href="http://cs.rochester.edu/~omidb/nlpcalendar/" target="_blank">here</a> to access unofficially official conference calendar for the fields of Computational Linguistics and Natural Language Processing

			</p>
			</div>

			
         
</div>

		<!-- footer section -->
		<div class = "navbar navbar-default">
			<div class = "container">
				<p class = "navbar-text pull-left">&copy Ashish Anand, Web Development Advisor: <a href="http://iitg.ernet.in/stud/p.saptarshi/" target="_blank">Saptarshi Pyne</a>, Web Developer: <a href="http://www.iitg.ernet.in/stud/d.aparajita/" target="_blank">Aparajita Dutta</a>.</p>
				<!--a href = "http://youtube.com/codersguide" class = "navbar-btn btn-danger btn pull-right">Subscribe on YouTube</a-->
			</div>
		</div>
     
		<!-- script section -->
		<!-- bootstrap and jquery scripts go here -->
		<script src = "http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
		<script src = "js/bootstrap.js"></script>

		</body>
</html>
